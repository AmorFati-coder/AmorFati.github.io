---
title: Spider 入门
date: 2021-05-13
categories: Spider
tags: 
- http
- html
- css
- requests
- urllib
---

初步学会抓取网页元素

<!--more-->

## 又是HTTP

> F12大法

- Network面板


| 列名      | 看法                              |
| --------- | --------------------------------- |
| name      | 看样子是截取了主域名后的一部分    |
| status    | 状态码（200，500.......）         |
| type      | 文档类型 为啥HTML非得搞个document |
| initiator | 请求源   看不懂                   |
| size      | 大小，如果有cache的话，会有标识   |
| time      | 响应时间                          |
| waterfall | 可视化？                          |

  - Header

    请求和响应比较详细的内容

    ### 请求

    > CRUD

    - 请求头

      cookies, UA, type之类的玩意

    - 请求体

      表单数据，正好这几天刚写完了表单

      注意类型

      | Content-Type                      | 提交数据的方式 |
      | --------------------------------- | -------------- |
      | application/x-www-form-urlencoded | 表单数据       |
      | multipart/form-data               | 表单文件上传   |
      | application/json                  | 序列化json数据 |
      | text/xml                          | XML数据        |

    ### 响应

    - 响应头

      时间（create,update,outdate）类型，cookie

    - 响应体

      正主来了，我们要解析的对象

    
    
    ## 网页基础
    
    ### HTML
    
- 标签

- 节点

  ### CSS

  > 加buff

- 使用link标签与html配合	

- CSS选择器（还是比较喜欢用xpath）

```css
#id .class .class_inner{
position: center;//页面布局
bottom: 40px;//距离底边距离
width: 100%;//宽度
height: 181px;//高度
}
```

  

### JS

- 动态交互

- script标签与html交互

## 基本库的使用

### urllib

> https://docs.python.org/3/library/urllib.html

- request

  模拟发送请求

  - `urlopen(url, data=None, [timeout, ], cafile=None, capath=None, cadefault=False, context=None)`

  - Request

    `class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`
    
  - Handler

- error

  - 各种error：HTTPError、URLError
  - 异常里的属性
    - 错误状态码、原因、headers

- parse

  处理url

  - `urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)` 

    > urlstring 必填，scheme协议，优先采取urlstring 里的协议

  - urlunparse

    > 合并

  - `urllib.parse.urlsplit`(*urlstring*, *scheme=''*, *allow_fragments=True*)

  - urlunsplit

    > 长度5

  - `urllib.parse.urljoin`(*base*, *url*, *allow_fragments=True*)

  - urlencode()

  > 字典转GET参数

  - parse_qs()

  > 转存成字典类型

  - parse_qsl()

  > 元组列表

- quote()

> 转中文为URL编码

- robotparser

  听起来挺有用，但实际上...
  
  ```python
  from urllib.robotparser import RobotFileParser
  from urllib.request import urlopen
  rp = RobotFileParser()
  rp.parse(urlopen('http://www.zhihu.com/robots.txt').read().decode('utf-8').split('\n'))
  print(rp.can_fetch('*', 'https://www.zhihu.com/topic/19661050/hot'))
  print(rp.can_fetch('*', "https://www.zhihu.com/question/448199854/answer/1882163142"))
  ```
  
  - set_url ：用来设置 robots.txt 文件的链接。如果在创建 RobotFileParser 对象时传入了链接，那么就不需要再使用这个方法设置了。
  - read：读取 robots.txt 文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。
    parse：用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。
  - can_fetch：该方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL。返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False.
  - mtime：返回的是上次抓取和分析 robots.txt 的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。
  - modified：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。

### requests







