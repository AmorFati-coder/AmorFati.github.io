---
title: Spider 入门
date: 2021-05-11
categories: Spider
tags: 
- http
- html
- css
- requests
- urllib
---

初步学会抓取网页元素

<!--more-->

## 又是HTTP

> F12大法

- Network面板


| 列名      | 看法                              |
| --------- | --------------------------------- |
| name      | 看样子是截取了主域名后的一部分    |
| status    | 状态码（200，500.......）         |
| type      | 文档类型 为啥HTML非得搞个document |
| initiator | 请求源   看不懂                   |
| size      | 大小，如果有cache的话，会有标识   |
| time      | 响应时间                          |
| waterfall | 可视化？                          |

  - Header

    请求和响应比较详细的内容

    ### 请求

    > CRUD

    - 请求头

      cookies, UA, type之类的玩意

    - 请求体

      表单数据，正好这几天刚写完了表单

      注意类型

      | Content-Type                      | 提交数据的方式 |
      | --------------------------------- | -------------- |
      | application/x-www-form-urlencoded | 表单数据       |
      | multipart/form-data               | 表单文件上传   |
      | application/json                  | 序列化json数据 |
      | text/xml                          | XML数据        |

    ### 响应

    - 响应头

      时间（create,update,outdate）类型，cookie

    - 响应体

      正主来了，我们要解析的对象

    
    
    ## 网页基础
    
    ### HTML
    
- 标签

- 节点

  ### CSS

  > 加buff

- 使用link标签与html配合	

- CSS选择器（还是比较喜欢用xpath）

```css
#id .class .class_inner{
position: center;//页面布局
bottom: 40px;//距离底边距离
width: 100%;//宽度
height: 181px;//高度
}
```

  

### JS

- 动态交互

- script标签与html交互

## 基本库的使用

### urllib

> https://docs.python.org/3/library/urllib.html

- request

  模拟发送请求

  - `urlopen(url, data=None, [timeout, ], cafile=None, capath=None, cadefault=False, context=None)`

  - Request

    `class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`
    
  - Handler

- error

  - 各种error：HTTPError、URLError
  - 异常里的属性
    - 错误状态码、原因、headers

- parse

  处理url

  - `urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)` 

    > urlstring 必填，scheme协议，优先采取urlstring 里的协议

  - urlunparse

    > 合并

  - `urllib.parse.``urlsplit`(*urlstring*, *scheme=''*, *allow_fragments=True*)

  - urlunsplit

    > 长度5

  - `urllib.parse.``urljoin`(*base*, *url*, *allow_fragments=True*)

  - 

  - urlencode()

- robotparser

  听起来挺有用，但实际上...

### requests







